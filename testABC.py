'''
Copyright 2018 Hernan Gelaf-Romer
University of Massachusetts Lowell - CS
ECNET Researcher

'''

from ecnet.server import Server
import pandas as pd

'''
A program created to compare the results of the hyperparameters that are generated by the artificial bee colony in comparison to those
that we have been using 'generically' all along. 

We will run the neural network twice, one with the generic values and the other time with the values extracted from the ABC program, and 
write the predicted values to CSV files. Then, the average error value will be taken for each molecule across all builds, and compared
to each other.

'''

'''
Here we run the server with the generic values that we have been using. These values are the ones that are not generated by the artificial
bee colony. 

'''
def run_server_generic():
    # Create server object
    sv = Server()
    sv.vars['learning_rate'] = 0.1
    sv.vars['valid_mdrmse_stop'] = 0.01
    sv.vars['valid_max_epochs'] = 1500
    sv.vars['valid_mdrmse_memory'] = 1000

    sv.create_save_env()

    sv.vars['data_split'] = [0.7, 0.3, 0.0]                # for ‘slv’ data, only split data for training (learning + validation)
    sv.import_data('cn_model_v1_slv.csv')                  # import the training data
    sv.fit_mlp_model_validation('shuffle_lv')

    sv.vars['data_split'] = [0.0, 0.0, 1.0]                # 100% of ‘st’ data will be put into the test set
    sv.import_data('cn_model_v1_st.csv')                   # import the static test set data
    sv.select_best('test')
    test_results = sv.use_mlp_model()

    # Output results to specified file
    sv.output_results(results=test_results, filename='test_results_generic_test_master.csv')

    # Calculates errors for the test set
    test_errors = sv.calc_error('rmse', 'r2', 'mean_abs_error', 'med_abs_error', dset = 'test')
    return test_errors

'''
Here we will run the server with the modified values that are obtained by running the ABC. Values can be changed depending on ABC results
and should be updated upon updating and increasing the accuracy of the artificial bee colony.

'''
def run_server_modified():
    # Create server object
    sv = Server()
    sv.vars['learning_rate'] = 0.06831226025740823
    sv.vars['valid_mdrmse_stop'] = 0.0013627967309420112
    sv.vars['valid_max_epochs'] = 1815
    sv.vars['valid_mdrmse_memory'] = 1934
    sv.vars['mlp_hidden_layers[0][0]'] = 13
    sv.vars['mlp_hidden_layers[1][0]'] = 12

    sv.create_save_env()

    sv.vars['data_split'] = [0.7, 0.3, 0.0]               # for ‘slv’ data, only split data for training (learning + validation)
    sv.import_data('cn_model_v1_slv.csv')                 # import the training data
    sv.fit_mlp_model_validation('shuffle_lv')

    sv.vars['data_split'] = [0.0, 0.0, 1.0]               # 100% of ‘st’ data will be put into the test set
    sv.import_data('cn_model_v1_st.csv')                  # import the static test set data
    sv.select_best('test')
    test_results = sv.use_mlp_model()

    # Output results to specified file
    sv.output_results(results=test_results, filename='test_results_modified_test_master.csv')

    # Calculates errors for the test set
    test_errors = sv.calc_error('rmse', 'r2', 'mean_abs_error', 'med_abs_error', dset = 'test')

    return test_errors


'''
After running the 10 builds/5 Nodes/75 trials, we have to calculate the average error score across the 10 builds, this will give us the 
most accurate representation of the static test sets that have already been done earlier in the year.

'''
def calculate_error(test_errors, str):
    avg = 0
    count = 0
    for val in test_errors['rmse']:
        avg+=val
        count+=1
    print(str, avg/count)

if __name__ == "__main__":

    generic_errors = run_server_generic()    # Create the CSV file for the generic values and return all errors
    modified_errors = run_server_modified()   # Create the CSV file for the modified values and return all errors
    calculate_error(generic_errors, 'Generic Error:')   # Extract RMSE across all builds, average them
    calculate_error(modfied_errors, 'Modified Error:')  # Extract RMSE across all builds, average them
